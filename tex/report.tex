\documentclass[11pt]{article}
\usepackage[margin=1.2in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{float}
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{nccmath}
\usepackage[most]{tcolorbox}
\newtcolorbox[auto counter]{problem}[1][]{%
    enhanced,
    breakable,
    colback=white,
    colbacktitle=white,
    coltitle=black,
    fonttitle=\bfseries,
    boxrule=.6pt,
    titlerule=.2pt,
    toptitle=3pt,
    bottomtitle=3pt,
    title=GitHub repository of this project}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{A Memory-Efficient Two-Pass Lanczos Algorithm}
\author{Luca Lombardo}
\date{}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\begin{document}
\maketitle

\begin{abstract}
    \noindent The two-pass Lanczos algorithm presents a memory-efficient alternative to the standard one-pass method for computing the action of a matrix function, $f(\mathbf{A})\mathbf{b}$. We analyze its theoretical complexities, confirming a memory reduction from $O(nk)$ to $O(n)$ in exchange for doubling the number of matrix-vector products. Numerical experiments reveal two key insights. First, despite the increased computational work, the two-pass method's wall-clock performance is often superior in practice due to its more favorable memory access patterns in cache-bound scenarios. Second, the algorithm maintains numerical accuracy equivalent to the one-pass approach, as the basis regeneration process does not introduce additional error, even for challenging ill-conditioned systems. These results establish the two-pass algorithm as a robust and often preferable practical alternative for large-scale computations.
\end{abstract}

{\setlength{\parskip}{0em}
\tableofcontents}

% ===================================================================
\section{Introduction}
% ===================================================================
The computation of the action of a matrix function on a vector, an operation denoted as $\mathbf{x} = f(\mathbf{A})\mathbf{b}$, represents a fundamental task in numerical linear algebra and scientific computing \cite{golub2013matrix, frommer2008matrix}. For large, sparse Hermitian matrices, methods based on Krylov subspaces are standard. A foundational algorithm in this class is the Lanczos process, first introduced as a \emph{method of minimized iterations} for eigenvalue problems \cite{lanczos1950iteration}. Its standard implementation, however, encounters a severe practical limitation: the necessity of storing an ever-growing basis of Lanczos vectors. This memory requirement often becomes prohibitive for the large-scale problems encountered in practice \cite{golub2013matrix}.

To address this memory bottleneck, we examine a simple and effective variant known as the two-pass Lanczos method \cite{frommer2008matrix}. This approach fundamentally alters the standard algorithm by separating the computation into two distinct phases. It first generates the projection of the matrix without storing the basis, and then regenerates the basis in a second pass to construct the final solution. This report provides an analysis of this method, focusing on the explicit trade-off it presents: a significant reduction in memory storage at the cost of an increased computational workload.

\subsection{The Problem of Computing Matrix Functions}
We consider the problem of computing the vector $\mathbf{x} \in \mathbb{C}^n$ defined by
\begin{equation*}
    \mathbf{x} = f(\mathbf{A})\mathbf{b},
\end{equation*}
where $\mathbf{A} \in \mathbb{C}^{n \times n}$ is a large, sparse Hermitian matrix and $f: \Omega \subseteq \mathbb{C} \to \mathbb{C}$ is a function defined on a set $\Omega$ containing the spectrum of $\mathbf{A}$, $\sigma(\mathbf{A})$. The most direct definition of a matrix function arises when the function $f$ is a polynomial. For a scalar polynomial $p(z) = \sum_{j=0}^m c_j z^j$, the corresponding matrix function $p(\mathbf{A})$ is defined by substituting the matrix $\mathbf{A}$ for the scalar variable $z$:
\begin{equation*}
    p(\mathbf{A}) = c_0\mathbf{I} + c_1\mathbf{A} + \dots + c_m\mathbf{A}^m.
\end{equation*}
This elementary case provides the foundation for defining $f(\mathbf{A})$ for more general functions. A key result from the theory of matrix functions is that for any function $f$ defined on the spectrum of $\mathbf{A}$, the matrix $f(\mathbf{A})$ is equivalent to a polynomial in $\mathbf{A}$.

\begin{proposition} \label{prop:matrix_function_polynomial}
    Let $f$ be a function defined on the spectrum of $\mathbf{A} \in \mathbb{C}^{n \times n}$. Then $f(\mathbf{A}) = p(\mathbf{A})$, where $p$ is the unique polynomial of minimal degree that interpolates $f$ and its derivatives on the spectrum of $\mathbf{A}$ in the Hermite sense.
    \begin{equation*}
        \frac{d^j p}{dz^j}(\lambda_i) = \frac{d^j f}{dz^j}(\lambda_i), \quad j = 0, \dots, m_i - 1,
    \end{equation*}
    for every distinct eigenvalue $\lambda_i \in \sigma(\mathbf{A})$, where $m_i$ is the size of the largest Jordan block associated with $\lambda_i$. The matrix function is then defined as $f(\mathbf{A}) := p(\mathbf{A})$.
\end{proposition}

The polynomial equivalence provides the foundation for the methods we consider. The computation of the matrix $f(\mathbf{A})$ itself, however, presents significant challenges. A direct approach might involve the spectral decomposition of the matrix, $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$, from which $f(\mathbf{A}) = \mathbf{V}f(\mathbf{\Lambda})\mathbf{V}^{-1}$. This method is only numerically viable if the matrix of eigenvectors $\mathbf{V}$ is well-conditioned. For general non-Hermitian matrices, or even for Hermitian matrices with clustered eigenvalues, this approach can suffer from a severe loss of accuracy.

A more stable method for computing $f(\mathbf{A})$ for small to medium-sized matrices is the Schur-Parlett algorithm. The algorithm begins by computing the Schur form of the matrix, $\mathbf{A} = \mathbf{Q}\mathbf{T}\mathbf{Q}^*$, where $\mathbf{Q}$ is unitary and $\mathbf{T}$ is upper triangular. The problem is then reduced to computing $f(\mathbf{T})$, after which the full matrix is recovered as $f(\mathbf{A}) = \mathbf{Q}f(\mathbf{T})\mathbf{Q}^*$. The asymptotic cost of this procedure is $O(n^3 + c_f n)$, where $c_f$ is the cost of evaluating the scalar function $f$. The cubic term arises from the initial Schur decomposition, rendering this approach impractical for the large-scale matrices.

A further impediment is that even when $\mathbf{A}$ is sparse, the resulting matrix $f(\mathbf{A})$ is generally dense. Storing this matrix would require $O(n^2)$ memory, which is often prohibitive. For these reasons, our focus is on methods that compute the action $\mathbf{x} = f(\mathbf{A})\mathbf{b}$ directly, without forming $f(\mathbf{A})$. This problem is an evergreen in many areas of scientific computing \cite{golub2013matrix}. A primary example is the solution of a large system of linear equations $\mathbf{Ax} = \mathbf{b}$, which corresponds to the case where $f(z) = z^{-1}$ and $\mathbf{A}$ is nonsingular \cite{saad2003iterative}. Another critical application arises in the numerical solution of systems of linear ordinary differential equations. The solution to the initial value problem $\mathbf{y}'(t) = \mathbf{A}\mathbf{y}(t)$, with $\mathbf{y}(0) = \mathbf{y}_0$, is given by $\mathbf{y}(t) = \exp(t\mathbf{A})\mathbf{y}_0$. This computation is an instance of our problem where the function is the matrix exponential, $f(z) = \exp(tz)$ \cite{frommer2008matrix}.

\subsection{Krylov Subspace Methods: The Standard Approach} \label{sec:krylov_methods}
The polynomial representation $f(\mathbf{A}) = p(\mathbf{A})$ seen in \ref{prop:matrix_function_polynomial} suggests that the solution vector $\mathbf{x} = f(\mathbf{A})\mathbf{b}$ can be constructed from a sequence of vectors generated by successive applications of the matrix $\mathbf{A}$ to the starting vector $\mathbf{b}$. This observation provides a direct path to the use of Krylov subspace methods.

\begin{definition}[Krylov Subspace]
    Given a matrix $\mathbf{A} \in \mathbb{C}^{n \times n}$ and a vector $\mathbf{b} \in \mathbb{C}^n$, the $k$-th Krylov subspace generated by $\mathbf{A}$ and $\mathbf{b}$ is the vector space
    \begin{equation*}
        \mathcal{K}_k(\mathbf{A}, \mathbf{b}) = \text{span}\{\mathbf{b}, \mathbf{Ab}, \dots, \mathbf{A}^{k-1}\mathbf{b}\}.
    \end{equation*}
    This space is equivalently characterized as the set of all vectors of the form $q(\mathbf{A})\mathbf{b}$, where $q$ is a polynomial of degree less than $k$ \cite{chen2024lanczos}.
\end{definition}

The standard method for computing an approximation to $\mathbf{x}$ from within $\mathcal{K}_k(\mathbf{A}, \mathbf{b})$ is the Arnoldi method, an iterative process that constructs an orthonormal basis for the subspace. After $k$ steps, this process generates a set of orthonormal vectors $\{\mathbf{v}_1, \dots, \mathbf{v}_k\}$ that form the columns of a matrix $\mathbf{V}_k \in \mathbb{C}^{n \times k}$. The underlying recurrence relation of the method can be summarized in the Arnoldi decomposition
\begin{equation} \label{eq:arnoldi_decomposition}
    \mathbf{A}\mathbf{V}_k = \mathbf{V}_k \mathbf{H}_k + h_{k+1,k} \mathbf{v}_{k+1} \mathbf{e}_k^T,
\end{equation}
where $\mathbf{H}_k = \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k$ is the $k \times k$ orthogonal projection of $\mathbf{A}$ onto the subspace $\mathcal{K}_k(\mathbf{A}, \mathbf{b})$. The structure of the Arnoldi process, which orthogonalizes each new vector only against the preceding ones, ensures that $\mathbf{H}_k$ is an upper Hessenberg matrix \cite{chen2024lanczos}.

The approximation for $f(\mathbf{A})\mathbf{b}$ is constructed as an extension of the Full Orthogonalization Method (FOM) for linear systems. The original problem is replaced by its projection onto the subspace, and the solution to this smaller problem is then lifted back to the original space. Assuming the starting vector is normalized as $\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2$, this yields the approximation
\begin{equation} \label{eq:arnoldi_approximation}
    \mathbf{x}_k = \|\mathbf{b}\|_2 \mathbf{V}_k f(\mathbf{H}_k) \mathbf{e}_1.
\end{equation}
This approach reduces the high-dimensional problem to the computation of the function of the small $k \times k$ matrix $\mathbf{H}_k$. This subproblem is computationally tractable, typically solved using methods like the Schur-Parlett algorithm at a cost of $O(k^3)$, which is negligible for $k \ll n$.

For the case of a Hermitian matrix $\mathbf{A}$, the Arnoldi process simplifies considerably. The projected matrix $\mathbf{H}_k = \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k$ must also be Hermitian:
\begin{equation*}
    (\mathbf{H}_k)^H = (\mathbf{V}_k^H \mathbf{A} \mathbf{V}_k)^H = \mathbf{V}_k^H \mathbf{A}^H \mathbf{V}_k = \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k = \mathbf{H}_k.
\end{equation*}
A matrix that is simultaneously upper Hessenberg and Hermitian must necessarily be a real, symmetric tridiagonal matrix \cite{chen2024lanczos}. This structural simplification allows the long recurrence of the Arnoldi method to be replaced by a much more efficient three-term recurrence. This specialized algorithm is the symmetric Lanczos process.

The projected matrix is thus a real, symmetric tridiagonal matrix, commonly denoted $\mathbf{T}_k$, and the final approximation takes the form
\begin{equation*}
    \mathbf{x}_k = \mathbf{V}_k f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2,
\end{equation*}
where $\mathbf{e}_1$ is the first canonical basis vector in $\mathbb{C}^k$ \cite{frommer2008matrix}.

The quality of this approximation is rigorously related to the error of the best uniform polynomial approximant of $f$ on the spectrum of $\mathbf{A}$. This relationship is formalized in the following theorem.

\begin{theorem}
    Let $\mathbf{A} \in \mathbb{C}^{n \times n}$ be a Hermitian matrix with spectrum $\sigma(\mathbf{A})$, and let $\mathbf{x}_k$ be the approximation to $\mathbf{x} = f(\mathbf{A})\mathbf{b}$ obtained after $k$ steps of the Lanczos algorithm. Then the error is bounded by
    \begin{equation*}
        \|\mathbf{x} - \mathbf{x}_k\|_2 \le 2\|\mathbf{b}\|_2 \min_{p \in \mathcal{P}_{k-1}} \max_{z \in \sigma(\mathbf{A})} |f(z) - p(z)|,
    \end{equation*}
    where $\mathcal{P}_{k-1}$ is the set of polynomials of degree at most $k-1$ \cite{chen2024lanczos}.
\end{theorem}

This bound demonstrates that the convergence of the Lanczos approximation depends directly on how well the function $f$ can be approximated by a low-degree polynomial over the spectral interval of $\mathbf{A}$.

\subsection{The Memory Bottleneck of the Standard Lanczos Process} \label{sec:memory_bottleneck}
The practical utility of approximating the solution via $\mathbf{x}_k = \mathbf{V}_k f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2$ is conditioned on our ability to construct and store the basis matrix $\mathbf{V}_k$. In a standard one-pass implementation, the Lanczos vectors $\{\mathbf{v}_j\}_{j=1}^k$ are generated sequentially and must all be retained in memory. This is because the final step involves forming a linear combination of these vectors, weighted by the components of the vector $\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2$.

This requirement imposes a memory cost that scales as $O(nk)$. For many problems of practical interest\footnote{particularly those arising from the discretization of partial differential equations} the dimension $n$ can be on the order of millions or larger. Concurrently, achieving a desired accuracy may require a number of iterations $k$ that is substantial, leading to a storage demand that exceeds the capacity of modern computing systems \cite{golub2013matrix}. This memory constraint is the primary bottleneck of the standard Lanczos process.

To overcome this limitation we need memory-efficient strategies. In section \ref{sec:two_pass_lanczos} we will discuss the two-pass Lanczos method, a simple but effective approach designed explicitly to address this problem of memory consumption \cite{frommer2008matrix}. The method is structured to avoid storing the entire basis $\mathbf{V}_k$ by decoupling the generation of the projected matrix $\mathbf{T}_k$ from the final synthesis of the solution vector $\mathbf{x}_k$.

\section{Symmetric Lanczos Process} \label{sec:lanczos_process}
To analyze the two-pass variant, we must first formalize the standard symmetric Lanczos process. As seen in section \ref{sec:krylov_methods}, the structure of this process is a direct consequence of applying the Arnoldi method to a Hermitian matrix $\mathbf{A}$. We begin from the Arnoldi decomposition, given in equation \eqref{eq:arnoldi_decomposition}. We have seen that for a Hermitian operator, this relation simplifies to the compact matrix equation
\begin{equation} \label{eq:lanczos_matrix_form}
    \mathbf{A}\mathbf{V}_k = \mathbf{V}_k \mathbf{T}_k + \beta_k \mathbf{v}_{k+1} \mathbf{e}_k^T,
\end{equation}
where $\mathbf{V}_k = [\mathbf{v}_1, \dots, \mathbf{v}_k] \in \mathbb{C}^{n \times k}$ is the matrix of orthonormal Lanczos vectors, $\mathbf{e}_k$ is the $k$-th canonical basis vector, and $\mathbf{T}_k$ is the $k \times k$ real symmetric tridiagonal matrix
\begin{equation*}
    \mathbf{T}_k =
    \begin{pmatrix}
        \alpha_1 & \beta_1  &             &             \\
        \beta_1  & \alpha_2 & \ddots      &             \\
                 & \ddots   & \ddots      & \beta_{k-1} \\
                 &          & \beta_{k-1} & \alpha_k
    \end{pmatrix}.
\end{equation*}
This matrix equation contains the set of vector relations that define the algorithm. By equating the $j$-th column of both sides of \eqref{eq:lanczos_matrix_form} for $j=1, \dots, k$, we can extract the underlying vector recurrence. The $j$-th column of the left-hand side is $\mathbf{A}\mathbf{v}_j$. The $j$-th column of the term $\mathbf{V}_k \mathbf{T}_k$ is given by the linear combination of the columns of $\mathbf{V}_k$ weighted by the entries of the $j$-th column of $\mathbf{T}_k$:
\begin{equation*}
    (\mathbf{V}_k \mathbf{T}_k)_j = \beta_{j-1}\mathbf{v}_{j-1} + \alpha_j\mathbf{v}_j + \beta_j\mathbf{v}_{j+1},
\end{equation*}
where we have defined the conventions $\beta_0=0$ and $\mathbf{v}_0 = \mathbf{0}$. For $j < k$, the term $\beta_k \mathbf{v}_{k+1} \mathbf{e}_k^T$ does not contribute to the $j$-th column. Equating the columns thus yields:
\begin{equation*}
    \mathbf{A}\mathbf{v}_j = \beta_{j-1}\mathbf{v}_{j-1} + \alpha_j\mathbf{v}_j + \beta_j\mathbf{v}_{j+1}.
\end{equation*}
Rearranging this expression gives us the three-term recurrence relation of the Lanczos process:
\begin{equation} \label{eq:lanczos_vector_recurrence}
    \beta_j \mathbf{v}_{j+1} = \mathbf{A}\mathbf{v}_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}.
\end{equation}
The real scalar coefficients are determined at each step by enforcing the orthonormality of the sequence $\{\mathbf{v}_j\}$. From the recurrence \eqref{eq:lanczos_vector_recurrence}, taking the inner product of both sides with $\mathbf{v}_j$ and using the orthogonality conditions $\mathbf{v}_j^H\mathbf{v}_{j+1}=0$ and $\mathbf{v}_j^H\mathbf{v}_{j-1}=0$ isolates $\alpha_j$:
\begin{equation*}
    \alpha_j = \mathbf{v}_j^H \mathbf{A} \mathbf{v}_j.
\end{equation*}
The off-diagonal elements, $\beta_j$, are determined from the normalization condition $\|\mathbf{v}_{j+1}\|_2 = 1$, which requires that $\beta_j$ be the norm of the unnormalized residual vector:
\begin{equation*}
    \beta_j = \|\mathbf{A}\mathbf{v}_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}\|_2.
\end{equation*}
If at some step $j < n$, $\beta_j = 0$, the algorithm terminates. This occurs if the subspace $\mathcal{K}_j(\mathbf{A}, \mathbf{b})$ is invariant under $\mathbf{A}$. Otherwise, for $j < n$, $\beta_j > 0$.

Finally, the matrix form \eqref{eq:lanczos_matrix_form} also confirms that $\mathbf{T}_k$ is the orthogonal projection of $\mathbf{A}$ onto the Krylov subspace. By left-multiplying by $\mathbf{V}_k^H$ and using the orthonormality conditions $\mathbf{V}_k^H \mathbf{V}_k = \mathbf{I}_k$ and $\mathbf{V}_k^H \mathbf{v}_{k+1} = \mathbf{0}$, we recover
\begin{equation*}
    \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k = \mathbf{V}_k^H (\mathbf{V}_k \mathbf{T}_k + \beta_k \mathbf{v}_{k+1} \mathbf{e}_k^T) = \mathbf{I}_k \mathbf{T}_k + \mathbf{0} = \mathbf{T}_k.
\end{equation*}
This set of relations forms the computational basis for both the standard and the two-pass Lanczos algorithms.

\section{The Two-Pass Lanczos Algorithm} \label{sec:two_pass_lanczos}
% ===================================================================
Having established the theoretical framework for the standard Lanczos process (section \ref{sec:lanczos_process}) and identified its principal limitation\footnote{the prohibitive memory cost of storing the basis vectors} we can now introduce a memory-efficient variant known as the two-pass Lanczos algorithm \cite{frommer2008matrix}. The algorithm resolves the memory constraint by decoupling the computation of the projected tridiagonal matrix $\mathbf{T}_k$ from the synthesis of the final solution vector $\mathbf{x}_k$.

In this section, we provide a description of the two distinct phases of the algorithm: an initial pass to generate the coefficients of $\mathbf{T}_k$ without storing the basis, and a second pass to reconstruct the solution vector. We then present an analysis of the method's core trade-off, quantifying its memory savings against its increased computational cost.

\subsection{Algorithmic Formulation}
The two-pass Lanczos algorithm computes the standard Lanczos approximation $\mathbf{x}_k$ while avoiding the $O(nk)$ memory cost associated with storing the basis matrix $\mathbf{V}_k$. It achieves this by executing two distinct computational passes. The first pass computes the tridiagonal projection $\mathbf{T}_k$, and the second pass synthesizes the solution vector $\mathbf{x}_k$.

\subsubsection{First Pass: Projection and Coefficient Generation}
The objective of the first pass is to compute the scalar entries of the $k \times k$ symmetric tridiagonal matrix $\mathbf{T}_k$. This is accomplished by executing $k$ steps of the symmetric Lanczos process, as detailed in Section \ref{sec:lanczos_process}

The algorithm iteratively generates the Lanczos coefficients $\{\alpha_j\}_{j=1}^k$ and $\{\beta_j\}_{j=1}^{k-1}$ using the three-term recurrence relation previously derived in equation \eqref{eq:lanczos_vector_recurrence}. The crucial distinction in this pass is the memory management strategy: while the recurrence requires the two most recent Lanczos vectors ($\mathbf{v}_j$ and $\mathbf{v}_{j-1}$) to compute the next, the full basis $\{\mathbf{v}_j\}_{j=1}^k$ is not stored. Only a constant number of vectors are kept in memory at any given time.

After $k$ iterations, the sequences of scalars are used to construct the matrix $\mathbf{T}_k$. The final operation of this pass is the computation of the coefficient vector $\mathbf{y}_k \in \mathbb{C}^k$:
\begin{equation}
    \mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2.
\end{equation}
This vector contains the coordinates of the approximate solution with respect to the (discarded) orthonormal basis $\mathbf{V}_k$. The computation of $f(\mathbf{T}_k)$ is a small-scale dense matrix problem whose cost is independent of $n$, and is thus considered negligible for $k \ll n$.

\subsubsection{Second Pass: Solution Reconstruction}
In the second pass, we construct the final solution vector $\mathbf{x}_k$. The vector $\mathbf{x}_k$ is the linear combination of the Lanczos basis vectors, where the coefficients are the components of the vector $\mathbf{y}_k$ computed in the first pass. We define the approximation as
\begin{equation}
    \mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k = \sum_{j=1}^k (\mathbf{y}_k)_j \mathbf{v}_j.
    \label{eq:solution_sum}
\end{equation}
To compute this sum without storing the matrix $\mathbf{V}_k$, we re-generate the Lanczos vectors sequentially \cite{frommer2008matrix}.

We re-initialize the process with the starting vector $\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2$. We then execute the Lanczos iteration a second time, using the scalars $\{\alpha_j\}_{j=1}^k$ and $\{\beta_j\}_{j=1}^{k-1}$ that we stored during the first pass. For each $j = 1, \dots, k-1$, we reconstruct the subsequent Lanczos vectors via the same three-term recurrence from equation \eqref{eq:lanczos_vector_recurrence}, rearranged to isolate $\mathbf{v}_{j+1}$:
\begin{equation}
    \mathbf{v}_{j+1} = \frac{1}{\beta_j} \left( \mathbf{A}\mathbf{v}_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1} \right).
    \label{eq:reconstruction_recurrence}
\end{equation}
As we re-generate each vector $\mathbf{v}_j$, we immediately use it to form the sum in equation \eqref{eq:solution_sum}. We initialize the solution vector $\mathbf{x}_k$ to zero. After computing $\mathbf{v}_1$, we form the first term of the sum. Then, for each $j=1, \dots, k-1$, after computing $\mathbf{v}_{j+1}$ via \eqref{eq:reconstruction_recurrence}, we update the solution as follows:
\begin{equation*}
    \mathbf{x}_k \leftarrow \mathbf{x}_k + (\mathbf{y}_k)_{j+1} \mathbf{v}_{j+1}.
\end{equation*}
The memory management is identical to that of the first pass. We use each vector $\mathbf{v}_j$ to compute $\mathbf{v}_{j+1}$ and to update the sum for $\mathbf{x}_k$. Afterwards, we retain it only as long as required for the next step of the recurrence. After $k$ steps, the accumulation is complete. The procedure is formally described in Algorithm \ref{alg:two_pass_lanczos}.

\begin{algorithm}[ht!]
    \caption{The Two-Pass Lanczos Method for $\mathbf{x} = f(\mathbf{A})\mathbf{b}$}
    \label{alg:two_pass_lanczos}
    \begin{algorithmic}
        \State \textbf{Input:} Hermitian matrix $\mathbf{A} \in \mathbb{C}^{n \times n}$, vector $\mathbf{b} \in \mathbb{C}^n$, function $f$, number of iterations $k$.
        \State \textbf{Output:} Approximate solution $\mathbf{x}_k \in \mathbb{C}^n$.

        \Statex \Comment{\textit{--- First Pass: Coefficient Generation ---}}
        \State Store $\mathbf{b}$ for the second pass. Let $\|\mathbf{b}\|_2$ be its norm.
        \State $\beta_0 \leftarrow 0$, $\mathbf{v}_{\text{prev}} \leftarrow \mathbf{0}$, $\mathbf{v} \leftarrow \mathbf{b} / \|\mathbf{b}\|_2$.
        \For{$j = 1, \dots, k$}
        \State $\mathbf{w} \leftarrow \mathbf{A}\mathbf{v} - \beta_{j-1}\mathbf{v}_{\text{prev}}$
        \State $\alpha_j \leftarrow \mathbf{v}^H \mathbf{w}$
        \State $\mathbf{w} \leftarrow \mathbf{w} - \alpha_j \mathbf{v}$
        \State $\beta_j \leftarrow \|\mathbf{w}\|_2$
        \State Store $\alpha_j$ and $\beta_j$.
        \State \textbf{if} $\beta_j = 0$ \textbf{then} break \textbf{end if}
        \State $\mathbf{v}_{\text{prev}} \leftarrow \mathbf{v}$
        \State $\mathbf{v} \leftarrow \mathbf{w} / \beta_j$
        \EndFor
        \State Let $k$ be the final iteration count.
        \State Construct $\mathbf{T}_k \in \mathbb{R}^{k \times k}$ from stored $\{\alpha_j\}_{j=1}^k$ and $\{\beta_j\}_{j=1}^{k-1}$.
        \State Compute coefficient vector $\mathbf{y}_k \leftarrow f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2$.

        \Statex \Comment{\textit{--- Second Pass: Solution Reconstruction ---}}
        \State $\beta_0 \leftarrow 0$, $\mathbf{v}_{\text{prev}} \leftarrow \mathbf{0}$, $\mathbf{v} \leftarrow \mathbf{b} / \|\mathbf{b}\|_2$.
        \State $\mathbf{x}_k \leftarrow (\mathbf{y}_k)_1 \mathbf{v}$.
        \For{$j = 1, \dots, k-1$}
        \State $\mathbf{w} \leftarrow \mathbf{A}\mathbf{v} - \alpha_j \mathbf{v} - \beta_{j-1}\mathbf{v}_{\text{prev}}$ \Comment{Use stored $\alpha_j$, $\beta_{j-1}$}
        \State $\mathbf{v}_{\text{next}} \leftarrow \mathbf{w} / \beta_j$ \Comment{Use stored $\beta_j$}
        \State $\mathbf{x}_k \leftarrow \mathbf{x}_k + (\mathbf{y}_k)_{j+1} \mathbf{v}_{\text{next}}$
        \State $\mathbf{v}_{\text{prev}} \leftarrow \mathbf{v}$
        \State $\mathbf{v} \leftarrow \mathbf{v}_{\text{next}}$
        \EndFor
        \State \textbf{return} $\mathbf{x}_k$.
    \end{algorithmic}
\end{algorithm}

\subsection{Computational and Memory Complexity} \label{sec:complexity}
We now provide a formal analysis of the memory requirements and computational cost of the two-pass Lanczos algorithm. To formalize this analysis, we model the total peak memory usage, $M(n, k)$, as a function of the problem dimension $n$ and the number of iterations $k$. This total memory can be decomposed into a base cost required for problem representation and an additional cost specific to the algorithm's execution:
\begin{equation}
    M(n, k) = M_{\text{base}}(n) + M_{\text{alg}}(n, k).
    \label{eq:memory_model}
\end{equation}
The base component, $M_{\text{base}}(n)$, is common to both algorithms. It includes the memory for storing the sparse matrix $\mathbf{A}$ and a constant number of work vectors. The term $M_{\text{alg}}(n, k)$ represents the additional memory specific to each algorithm's execution strategy, which is the source of the substantial difference between the two methods.

\begin{proposition}[Memory Complexity]
    Let $n$ be the dimension of the matrix $\mathbf{A}$ and $k$ be the number of iterations. The standard one-pass Lanczos algorithm has a memory complexity of $O(nk)$, while the two-pass variant reduces this requirement to $O(n)$.
\end{proposition}
\begin{proof}
    The base memory component, $M_{\text{base}}(n)$, includes storage for the sparse matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ and a constant number of work vectors, $c_v$, each of dimension $n$. Let $\text{nnz}(\mathbf{A})$ be the number of non-zero entries in $\mathbf{A}$, and let $s_d$ be the size of a double-precision float. The storage for $\mathbf{A}$ in a Compressed Sparse Column (CSC) format requires approximately $s_d(\text{nnz}(\mathbf{A}) + n) + s_i(\text{nnz}(\mathbf{A}))$, where $s_i$ is the size of an integer index. For the class of sparse problems considered, $\text{nnz}(\mathbf{A}) \propto n$. The base memory cost is therefore strictly linear in $n$, so $M_{\text{base}}(n) = O(n)$.

    The difference between the methods lies in the algorithm-specific term, $M_{\text{alg}}(n, k)$. The standard one-pass algorithm must store the entire basis matrix $\mathbf{V}_k \in \mathbb{R}^{n \times k}$ to synthesize the final solution. This imposes a significant memory overhead:
    \begin{equation*}
        M_{\text{alg, std}}(n, k) = n \cdot k \cdot s_d = O(nk).
    \end{equation*}
    The total memory is therefore $M_{\text{std}}(n, k) = M_{\text{base}}(n) + M_{\text{alg, std}}(n, k) = O(n) + O(nk) = O(nk)$.

    The two-pass algorithm is designed to minimize this term. It stores only the scalar coefficients of $\mathbf{T}_k$, resulting in a negligible cost of $M_{\text{alg, TP}}(k) = O(k)$. The total memory requirement is thus:
    $$M_{\text{TP}}(n, k) = M_{\text{base}}(n) + M_{\text{alg, TP}}(k) = O(n) + O(k)$$
    Under the common assumption that $n \gg k$, this simplifies to $O(n)$.
\end{proof}

\begin{proposition}[Computational Complexity]
    Let the dominant computational cost of the Lanczos process be the matrix-vector products. For $k$ iterations, the standard one-pass Lanczos algorithm requires $k$ matrix-vector products, whereas the two-pass algorithm requires $2k$.
\end{proposition}
\begin{proof}
    For large, sparse matrices, the matrix-vector product $\mathbf{A}\mathbf{v}_j$ is the most computationally expensive operation per iteration \cite{saad2003iterative}. The standard one-pass method performs one such product at each of its $k$ iterations, for a total of $k$ products.

    The two-pass method executes two distinct passes. The first pass requires $k$ matrix-vector products to generate the coefficients of $\mathbf{T}_k$. To synthesize the solution, the second pass must re-generate the Lanczos basis vectors, which forces the re-computation of the same sequence of $k$ matrix-vector products. The total count is therefore $2k$.
\end{proof}

% \subsection{Alternative Methods}
% We contextualize the two-pass method by comparing it to an alternative strategy for achieving memory efficiency: the derivation of short-term recurrences for the solution vector itself. For certain specific choices of the function $f$, it is possible to algebraically manipulate the underlying Lanczos process to yield a direct update formula for the approximate solution $\mathbf{x}_k$.

% The most prominent example of such a method is the Conjugate Gradient (CG) algorithm, which applies to the solution of Hermitian positive definite linear systems $\mathbf{Ax} = \mathbf{b}$, a case corresponding to the matrix function $f(z) = z^{-1}$ \cite{saad2003iterative}. The CG method computes the approximation at step $k$ via a direct update of the form
% \begin{equation}
%     \mathbf{x}_k = \mathbf{x}_{k-1} + z_k \mathbf{p}_{k-1},
% \end{equation}
% where the search direction vector $\mathbf{p}_{k-1}$ is itself updated using a short-term recurrence involving the residual vector. As detailed in \cite{saad2003iterative}, these recurrences are a direct algebraic consequence of the symmetric Lanczos process. This approach avoids storing the full basis $\mathbf{V}_k$ and is computationally very efficient for its specific task.

% In contrast, the two-pass method we have described offers a more general framework. Its applicability is not restricted to a particular function $f$. The method's structure, which separates the projection from the reconstruction, is universal. The only function-specific operation is the computation of the small coefficient vector $\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2$ \cite{frommer2008matrix}. This allows the same two-pass procedure to be applied to any function for which $f(\mathbf{T}_k)$ can be computed, including the matrix exponential, inverse square root, or other functions arising in applications \cite{boricci2000fast, casulli2025low}.

% This separation of concerns also leads to a conceptually direct implementation. The algorithm relies on the repeated application of the standard Lanczos recurrence, whose numerical properties are well-understood \cite{golub2013matrix}. It avoids the need to derive and implement function-specific update formulas for the high-dimensional solution and auxiliary vectors, which might have different numerical stability properties. The two-pass strategy therefore provides a simple and broadly applicable technique for addressing the memory limitations of the standard one-pass Lanczos process.

% ===================================================================

\section{Numerical Experiments}
% ===================================================================
In this section, we present a series of numerical experiments designed to validate the theoretical analysis presented in \ref{sec:complexity} and to assess the practical performance of the two-pass Lanczos algorithm. Our investigation is structured into three distinct experiments. The first validates the memory-versus-computation trade-off as a function of iteration count. The second assesses the scalability of both algorithms with respect to the problem dimension. The third provides an analysis of numerical stability and accuracy across several challenging problem scenarios.

\subsection{Experimental Setup and Performance Metrics}
All experiments were executed on a dual-socket machine running Ubuntu 22.04.2 LTS (GNU/Linux kernel 5.15.0-119-generic). The system is equipped with two Intel(R) Xeon(R) Gold 5318Y CPUs, each operating at a base frequency of 2.10GHz, for a total of 96 logical cores\footnote{however, for this experiments, all parallelism has been disabled}. The algorithms were implemented in Rust and compiled with \texttt{rustc} version 1.90.0 using release-level optimizations. All computations were performed using double-precision floating-point arithmetic.

We defined a set of quantitative metrics. The primary measure of computational work is the total number of matrix-vector products, which provides a hardware-independent basis for comparison \cite{saad2003iterative}, as it is the dominant operation in Krylov subspace methods for large, sparse matrices. We also report the total wall-clock time as a practical measure of performance. The memory footprint is quantified by the Peak Resident Set Size, corresponding to the \texttt{VmPeak} field in the \texttt{/proc/self/status} virtual file on our Linux system.

We measure the accuracy of a computed solution $\mathbf{x}_k$ by its relative error with respect to a known ground-truth solution $\mathbf{x}_{\text{true}}$. We assess numerical stability through two primary metrics. First, we quantify the loss of orthogonality of the Lanczos basis $\mathbf{V}_k$ via the Frobenius norm $\|I - \mathbf{V}_k^H \mathbf{V}_k\|_F$, a known phenomenon in finite-precision implementations of the process \cite{golub2013matrix}. Second, to confirm that the two-pass regeneration is stable, we measure the numerical equivalence of the final solutions by computing the L2-norm of their deviation, $\|\mathbf{x}_k - \mathbf{x}'_k\|_2$.

\subsection{Test Problem Generation}
For the performance and scalability experiments, we derive our test problems from Karush-Kuhn-Tucker (KKT) systems associated with convex quadratic separable Min-Cost Flow problems. Such systems are sparse, symmetric, and exhibit the block saddle-point structure
\begin{equation}
    A =
    \begin{pmatrix}
        D & E^T \\
        E & 0
    \end{pmatrix} \in \mathbb{R}^{n \times n}.
\end{equation}
We construct the node-arc incidence matrix, $E$, from network topologies generated by the \texttt{netgen} utility, which allows for precise control over problem dimension and sparsity. The diagonal block $D$ is defined as $D = \text{diag}(d_1, \dots, d_m)$, with its entries drawn from a uniform random distribution, $d_i \sim U[1, C_D]$.

This construction provides two key advantages for creating robust test cases. First, by setting $d_i \ge 1$, we ensure that $D$ is a symmetric positive definite matrix. This choice guarantees that the resulting KKT matrix $A$ is indefinite, which can be verified by examining the quadratic form $x^T A x$. Operating on indefinite matrices provides a robust test for the Lanczos algorithm, as the process does not require positive definiteness, a property that distinguishes it from methods such as the Conjugate Gradient algorithm \cite{golub2013matrix}.

Second, the parameter $C_D$ gives us control over the spectral properties of the problem. For a symmetric positive definite matrix, the 2-norm condition number is the ratio of its largest to its smallest eigenvalue, $\kappa_2(D) = \lambda_{\max}(D) / \lambda_{\min}(D)$ \cite{saad2003iterative}. Our construction thus yields $\kappa_2(D) \approx C_D$. The spectral properties of the operator are known to fundamentally govern the convergence rate of Krylov subspace methods \cite{golub2013matrix}. By varying $C_D$, we can therefore generate both well-conditioned and ill-conditioned problem instances.

For these performance tests, we adopt a known-solution methodology. We first define a ground-truth solution vector $\mathbf{x}_{\text{true}}$ and then construct the right-hand side vector as $\mathbf{b} := \mathbf{A} \mathbf{x}_{\text{true}}$.

While the KKT systems provide realistic, large-scale test cases for performance, a different approach is necessary for the numerical stability and accuracy analysis, which requires a precisely known analytical ground-truth solution for $f(\mathbf{A})\mathbf{b}$. For that experiment, we employ synthetic diagonal matrices, where the action of $f(\mathbf{A})$ can be computed to machine precision. This methodology is detailed further in Section \ref{sec:exp_stability}.

\subsection{Memory and Computation Trade-off}
\label{sec:exp1}
The first experiment was designed to validate the theoretical complexity analysis presented in Section \ref{sec:complexity}. The goal was to demonstrate the trade-off between memory consumption and computational workload by comparing the one-pass and two-pass Lanczos algorithms.

Based on our model, we formulate two distinct hypotheses. First, concerning memory usage, we expect the peak resident set size of the standard algorithm to exhibit linear growth with respect to the iteration count $k$, consistent with its $O(nk)$ complexity derived from storing the basis $\mathbf{V}_k$. On the other hand, we expect the two-pass method to maintain a nearly constant memory footprint, consistent with its $O(n)$ complexity. Second, regarding computational time, the model based on floating-point operations predicts that the wall-clock time for the two-pass method should be approximately double that of the one-pass method, due to the doubled number of matrix-vector products. However, this model neglects the cost of memory access. A more realistic hypothesis is that the actual performance will be governed by the relation between the additional arithmetic operations and the algorithms' memory access patterns. For problem sizes where the basis matrix $\mathbf{V}_k$ is too large to fit in processor caches, the cost of memory bandwidth may dominate, reducing the observable time penalty for the two-pass method.

We conducted this experiment using several KKT system instances of fixed dimension $n$ and varying sparsity. For each instance, we executed both algorithms while varying the number of iterations $k$ across a predefined range, recording the peak RSS and the total wall-clock time for each run.

\subsubsection{Results}
The memory consumption results, presented in Figures \ref{fig:large_scale_memory}, \ref{fig:medium_scale_memory}, and \ref{fig:small_scale_memory}, provide strong empirical support for the complexity analysis across all problem scales. The standard one-pass algorithm exhibits a linear growth in memory usage with respect to the iteration count $k$. This behavior stems directly from the need to store the entire basis matrix $\mathbf{V}_k \in \mathbb{C}^{n \times k}$, a requirement with a cost of $O(nk)$ \cite{golub2013matrix}. In contrast, the two-pass algorithm maintains a constant memory footprint, independent of $k$. This aligns with its theoretical $O(n)$ memory complexity, as the algorithm only requires simultaneous storage for a small, constant number of $n$-dimensional vectors.

A closer analysis of Figure \ref{fig:medium_scale_memory}, however, reveals a subtler dynamic. For a low number of iterations, the memory consumption of the two-pass method is slightly higher than that of the standard method, with a crossover point observed around $k \approx 300$. This phenomenon does not contradict the theoretical model but rather clarifies the constants it abstracts. Our model, $M(n, k) = M_{\text{base}}(n) + M_{\text{alg}}(n, k)$, remains valid, but the base cost, $M_{\text{base}}(n)$, is not identical for the two algorithms. The two-pass method incurs a slightly larger fixed memory overhead. This overhead arises from the need to store a copy of the initial vector $\mathbf{b}$ for the second pass, as well as the arrays for the scalar coefficients $\{\alpha_j\}$, $\{\beta_j\}$, and the solution vector $\mathbf{y}_k$. For small values of $k$, the storage cost for the basis in the standard method, $O(nk)$, is less than this fixed overhead. As $k$ increases, however, the $O(nk)$ term rapidly dominates.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs500k_rho3_memory.pdf}
        \caption{Peak memory usage for a large-scale problem (500k arcs).}
        \label{fig:large_scale_memory}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs500k_rho3_time.pdf}
        \caption{Wall-clock time for a large-scale problem (500k arcs).}
        \label{fig:large_scale_time}
    \end{minipage}
\end{figure}

For the large-scale instance with 500k arcs, shown in Figure \ref{fig:large_scale_time}, the wall-clock time of the two-pass method is only marginally greater than that of the one-pass method. This observation appears to contradict the expectation of doubled work \cite{frommer2008matrix}. The discrepancy arises because for very large $n$, the dominant cost shifts from sparse matrix-vector products to dense vector operations during solution reconstruction. Both the final matrix-vector product $\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k$ in the one-pass method and the incremental accumulation in the two-pass method have a complexity of $O(nk)$. When $\mathbf{V}_k$ is too large to fit in any CPU cache, these operations become limited by main memory bandwidth. This large, common cost term dominates the total execution time for both methods and masks the effect of the doubled sparse matrix-vector products.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs50k_rho3_memory.pdf}
        \caption{Peak memory usage for a medium-scale problem (50k arcs).}
        \label{fig:medium_scale_memory}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs50k_rho3_time.pdf}
        \caption{Wall-clock time for a medium-scale problem (50k arcs).}
        \label{fig:medium_scale_time}
    \end{minipage}
\end{figure}

As we reduce the problem dimension, this dynamic changes. For the medium-scale instance with 50k arcs, the results in Figure \ref{fig:medium_scale_time} show that the two-pass method is consistently faster than the standard one-pass method. This performance reversal originates from the memory access patterns of the algorithms. The standard algorithm's reconstruction step accesses a large contiguous block of memory for $\mathbf{V}_k$, generating an inefficient access pattern with poor data locality that leads to a high rate of cache misses. The two-pass algorithm, by contrast, operates on a small working set of vectors at each step. The processor's cache hierarchy manages this small working set effectively, resulting in a high cache hit rate and superior performance.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs5k_rho3_memory.pdf}
        \caption{Peak memory usage for a small-scale problem (5k arcs).}
        \label{fig:small_scale_memory}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/arcs5k_rho3_time.pdf}
        \caption{Wall-clock time for a small-scale problem (5k arcs).}
        \label{fig:small_scale_time}
    \end{minipage}
\end{figure}

This effect becomes even more pronounced for the small-scale instance with 5k arcs, shown in Figure \ref{fig:small_scale_time}. Here, the working set of the two-pass method can reside almost entirely within the fastest L1 and L2 caches, maximizing its performance advantage. In this cache-friendly regime, the performance gain from eliminating cache misses far outweighs the cost of the additional matrix-vector products.

\subsection{Scalability with Problem Dimension}
\label{sec:exp_scalability}
The second experiment assesses how the resource requirements of both algorithms scale with the problem dimension $n$. For this test, we fix the number of iterations $k$ to a constant value and generate a family of test matrices of increasing dimension. The network topologies are constructed to maintain a comparable sparsity across all instances. We hypothesize that the memory advantage of the two-pass algorithm will become increasingly significant as $n$ grows. This should manifest as a substantially steeper slope in the memory usage plot for the one-pass method, reflecting its linear dependence on the product $nk$, compared to a gentler slope for the two-pass method, which depends only linearly on $n$.

\subsubsection{Results}
We fixed the number of iterations at $k=500$ and analyzed the performance of both algorithms as a function of the problem dimension, $n$. Figures \ref{fig:scalability_memory} and \ref{fig:scalability_time} plots the peak memory usage and wall-clock time against $n$.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/scalability_memory.pdf}
        \caption{Peak memory usage vs. problem dimension $n$ for a fixed number of iterations ($k=500$), illustrating the significant $O(nk)$ cost of the standard method.}
        \label{fig:scalability_memory}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/scalability_time.pdf}
        \caption{Wall-clock time vs. problem dimension $n$ for a fixed number of iterations ($k=500$), showing linear scaling for both algorithms.}
        \label{fig:scalability_time}
    \end{minipage}
\end{figure}

The memory consumption profiles in Figure \ref{fig:scalability_memory} show a stark contrast between the two methods, confirming our hypothesis. The memory usage of both methods grows linearly with $n$, but the slope of this growth differs significantly. The two-pass method exhibits a gentle slope, corresponding to the linear growth of the base memory requirement, $M_{\text{base}}(n)$. The standard method, in contrast, shows a much steeper slope, reflecting the combined cost of the base memory and the algorithm-specific term for storing the basis, $M_{\text{std}}(n,k) = M_{\text{base}}(n) + M_{\text{alg, std}}(n, k)$.

In Table \ref{tab:scalability_data}, we provide the raw data underlying the memory plot. The memory usage of the two-pass method serves as a stable empirical baseline for $M_{\text{base}}(n)$. The final column, representing the difference in memory consumption, effectively isolates the cost of storing the basis matrix $\mathbf{V}_k$. A linear regression performed on this column's data yields a growth rate of approximately $3.97$ MB per 1000 units of increase in $n$, which corresponds to 4065 bytes per unit $n$. This observed rate aligns remarkably well with the theoretical rate predicted by our model, which is $k \cdot s_d = 500 \cdot 8 = 4000$ bytes per unit $n$.

\begin{table}[ht!]
    \centering
    \begin{tabular}{rrrr}
        \hline
        \multicolumn{1}{c}{\textbf{Dimension ($n$)}} & \multicolumn{1}{c}{\textbf{Standard (MB)}} & \multicolumn{1}{c}{\textbf{Two-Pass (MB)}} & \multicolumn{1}{c}{\textbf{Difference (MB)}} \\
        \hline
        50,365                                       & 6798.4                                     & 6352.3                                     & 446.1                                        \\
        100,516                                      & 6739.9                                     & 6419.7                                     & 320.2                                        \\
        150,632                                      & 6938.8                                     & 6428.2                                     & 510.6                                        \\
        200,730                                      & 7133.7                                     & 6430.4                                     & 703.3                                        \\
        250,816                                      & 7330.5                                     & 6435.8                                     & 894.7                                        \\
        300,894                                      & 7531.2                                     & 6447.3                                     & 1083.9                                       \\
        350,966                                      & 7726.6                                     & 6395.8                                     & 1330.8                                       \\
        401,033                                      & 7920.7                                     & 6451.8                                     & 1468.9                                       \\
        451,095                                      & 8117.4                                     & 6457.1                                     & 1660.3                                       \\
        501,155                                      & 8314.1                                     & 6526.4                                     & 1787.7                                       \\
        \hline
    \end{tabular}
    \caption{Peak memory usage (RSS) in megabytes for a fixed number of iterations ($k=500$). The difference isolates the memory cost of the basis matrix $\mathbf{V}_k$ in the standard algorithm.}
    \label{tab:scalability_data}
\end{table}

The wall-clock time for both algorithms, shown in Figure \ref{fig:scalability_time}, scales linearly with the problem dimension $n$. This is consistent with the theoretical cost, which is dominated by $k$ iterations of operations (such as sparse matrix-vector products) whose complexity is linear in $n$. With $k$ held constant, the total time complexity is $O(nk)$, resulting in the observed linear scaling. The execution times for both methods remain close, which is consistent with the findings from Section \ref{sec:exp1} for large-scale problems where memory bandwidth becomes a significant factor in overall performance.

\subsection{Numerical Accuracy and Stability Analysis}
\label{sec:exp_stability}
In our final experiment, we investigate the numerical accuracy and stability of the two-pass Lanczos method. Our primary objective is to verify that the solution computed via basis regeneration is numerically equivalent to the solution from the standard one-pass algorithm across a range of distinct scenarios. This analysis addresses two fundamental questions: what is the absolute accuracy of the Lanczos approximation for a given problem, and does the two-pass method maintain this accuracy, or does the process of regenerating the basis vectors introduce a significant numerical degradation?

To isolate the factors governing algorithmic performance, we designed four distinct experimental scenarios built upon synthetic diagonal test matrices. This approach allows us to exercise precise control over the problem's characteristics and to compute the ground-truth solution, $\mathbf{x}_{\text{true}} = f(\mathbf{A})\mathbf{b}$, to machine precision. We measure the relative error of the computed solutions $\mathbf{x}_k$ (one-pass) and $\mathbf{x}'_k$ (two-pass) against this reference. We also track the direct deviation $\|\mathbf{x}_k - \mathbf{x}'_k\|_2$ to quantify their numerical equivalence. Finally, we assess the stability of the basis generation by quantifying the loss of orthogonality of the Lanczos basis $V_k$ via the Frobenius norm $\|I - V_k^H V_k\|_F$, a known phenomenon in finite-precision implementations of the process \cite{golub2013matrix}.

\subsubsection{Exponential on a Well-Conditioned Spectrum}
We first test $f(z) = \exp(z)$ on a matrix $\mathbf{A}$ whose spectrum is a compact interval on the negative real axis, e.g., $\sigma(\mathbf{A}) \subset [-C, -c]$ with $C, c > 0$. As an entire function, the exponential is analytic everywhere. The theory of polynomial approximation predicts that for such functions on a compact set, the error of the Lanczos approximation converges super-linearly to zero \cite{frommer2008matrix}. This scenario serves as a baseline control experiment. In the absence of any problem-induced instabilities, we hypothesize that both Lanczos variants will converge rapidly to the ground truth and that their solutions will be numerically indistinguishable.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/accuracy_exp_well-conditioned.pdf}
        \caption{Accuracy for $f(z) = \exp(z)$ on a well-conditioned matrix.}
        \label{fig:acc_exp_well}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/orthogonality_exp_well-conditioned.pdf}
        \caption{Orthogonality loss for $f(z) = \exp(z)$ on a well-conditioned matrix.}
        \label{fig:ortho_exp_well}
    \end{minipage}
\end{figure}

The results for this scenario, presented in Figure \ref{fig:acc_exp_well}, confirm this hypothesis. As predicted by theory, the relative error for both methods converges super-linearly, reaching the level of machine precision in fewer than 30 iterations. The lower panel shows that the deviation between the two solutions, $\|\mathbf{x}_k - \mathbf{x}'_k\|_2$, remains stable and of the order of machine epsilon ($10^{-15}$), confirming that the regeneration process is intrinsically stable. This stability is reflected in the analysis of the basis itself, shown in Figure \ref{fig:ortho_exp_well}. The loss of orthogonality increases predictably with the number of iterations, but the profiles for the standard and regenerated bases are computationally identical. This outcome is a direct consequence of the deterministic nature of the algorithm when executed in finite-precision arithmetic. The second pass reconstructs the basis using the recurrence from Equation \eqref{eq:reconstruction_recurrence}, which is supplied with the exact scalar coefficients $\{\alpha_j, \beta_j\}$ stored during the first pass. Since the sequence of floating-point operations and their respective operands is identical in both the original generation and the regeneration, the resulting basis matrices $\mathbf{V}_k$ and $\mathbf{V}'_k$ are computationally indistinguishable.

\subsubsection{Inverse on a Well-Conditioned Spectrum}
Next, we test $f(z) = z^{-1}$ on a symmetric positive definite matrix $\mathbf{A}$ whose spectrum is strictly bounded away from zero, i.e., $\sigma(\mathbf{A}) \subset [c, C]$ with $0 < c$. This recasts the problem as the solution of a well-conditioned linear system. The classical theory of the conjugate gradient method, which is algebraically equivalent to Lanczos for this problem, predicts a monotonic error decrease, with a convergence rate governed by the condition number $\kappa(\mathbf{A})$ \cite{saad2003iterative}. This experiment validates the standard convergence behaviour and verifies the numerical equivalence of the two methods for a function with a singularity, provided the spectrum is kept in a safe region.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/accuracy_inv_well-conditioned.pdf}
        \caption{Accuracy for $f(z) = z^{-1}$ on a well-conditioned, positive definite matrix.}
        \label{fig:acc_inv_well}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/orthogonality_inv_well-conditioned.pdf}
        \caption{Orthogonality loss for $f(z) = z^{-1}$ on a well-conditioned matrix.}
        \label{fig:ortho_inv_well}
    \end{minipage}
\end{figure}

Figure \ref{fig:acc_inv_well} presents the results for this case. The convergence is linear and the error decreases monotonically until it stagnates at the level of machine precision. Again, the two methods produce nearly identical error curves, and their direct deviation is negligible. The underlying basis stability, shown in Figure \ref{fig:ortho_inv_well}, mirrors this result. The loss of orthogonality grows steadily but remains numerically identical for both the stored and regenerated bases, reinforcing the conclusion that the two-pass approach does not compromise accuracy on well-posed problems.

\subsubsection{Exponential on an Ill-Conditioned Spectrum}
We then test $f(z) = \exp(z)$ on a matrix with a very wide spectrum, e.g., $\sigma(\mathbf{A}) \subset [-C, -c]$ where $C \gg c$. While the problem remains well-posed, approximating $\exp(z)$ with a single polynomial over a large interval is known to be difficult, requiring a high polynomial degree $k$. This scenario tests the robustness of the methods under conditions of slow convergence. We hypothesize that both algorithms will exhibit identical, albeit slower, convergence, demonstrating that the two-pass approach remains stable even over a large number of iterations.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/accuracy_exp_ill-conditioned.pdf}
        \caption{Accuracy for $f(z) = \exp(z)$ on a matrix with a wide spectrum.}
        \label{fig:acc_exp_ill}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/orthogonality_exp_ill-conditioned.pdf}
        \caption{Orthogonality loss for $f(z) = \exp(z)$ on an ill-conditioned matrix.}
        \label{fig:ortho_exp_ill}
    \end{minipage}
\end{figure}

The results in Figure \ref{fig:acc_exp_ill} show a significantly slower convergence rate compared to the well-conditioned case, an expected consequence of the approximation challenge. Nonetheless, the convergence remains monotonic, and the two methods continue to produce numerically close results. The analysis of the basis in Figure \ref{fig:ortho_exp_ill} confirms that the stability of the two-pass method holds even when a large number of iterations are required. The loss of orthogonality is more pronounced than in the well-conditioned case, but the profiles for the standard and regenerated bases remain indistinguishable.

\subsubsection{Inverse on an Ill-Conditioned Spectrum}
Finally, we test $f(z) = z^{-1}$ on a symmetric, indefinite matrix $\mathbf{A}$ constructed to be nearly singular, with an eigenvalue $\lambda_j$ satisfying $|\lambda_j| \ll 1$. This provides a profound stress test for the algorithm. The Lanczos process is known to approximate extremal eigenvalues of $\mathbf{A}$ well \cite{golub2013matrix}. Consequently, the projected matrix $\mathbf{T}_k$ is expected to become severely ill-conditioned as one of its eigenvalues converges to $\lambda_j$. The computation of $\mathbf{y}_k = \mathbf{T}_k^{-1} \mathbf{e}_1 \|\mathbf{b}\|_2$ is therefore expected to be numerically unstable. We hypothesize that both methods will exhibit identical behaviour, demonstrating that any observed instability is an intrinsic property of the projected problem, not a flaw introduced by the two-pass reconstruction.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/accuracy_inv_ill-conditioned.pdf}
        \caption{Accuracy for $f(z) = z^{-1}$ on a nearly singular, indefinite matrix.}
        \label{fig:acc_inv_ill}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../results/images/orthogonality_inv_ill-conditioned.pdf}
        \caption{Orthogonality loss for $f(z) = z^{-1}$ on an ill-conditioned matrix.}
        \label{fig:ortho_inv_ill}
    \end{minipage}
\end{figure}

Figure \ref{fig:acc_inv_ill} illustrates this challenging scenario. The error plot shows three distinct phases. Initially, the error stagnates as the Krylov subspace fails to capture the problematic near-null space of $\mathbf{A}$. Subsequently, as the Lanczos process begins to approximate the near-zero eigenvalue, the projected matrix $\mathbf{T}_k$ becomes severely ill-conditioned. This induces a period of erratic, non-monotonic error behaviour, as the inversion of $\mathbf{T}_k$ becomes numerically unstable \cite{golub2013matrix}. Critically, this instability affects both methods identically. Finally, as $k$ becomes sufficiently large, the finite-termination property of the Lanczos method for linear systems comes into effect \cite{saad2003iterative}. The Krylov subspace becomes rich enough to construct a highly accurate polynomial approximant for $z^{-1}$, and the error converges abruptly to a high level of precision. Throughout this entire process, the solutions from the one-pass and two-pass methods remain numerically close. The basis analysis in Figure \ref{fig:ortho_inv_ill} confirms this finding. The severe loss of orthogonality observed coincides with the period of erratic error behavior in the corresponding solution accuracy plot, affirming that the instability is an intrinsic property of the Lanczos method applied to a nearly singular problem, which the two-pass algorithm correctly and faithfully reproduces.


\bibliographystyle{unsrt}
\bibliography{ref}
\nocite{*}



\end{document}
