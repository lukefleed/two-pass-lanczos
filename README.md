# Two-Pass Lanczos Implementation

This document provides an overview of the Two-Pass Lanczos implementation, including compilation instructions, optimization strategies, dependencies, repository structure, data generation, correctness testing, and reproduction of experimental results.

## Install Rust

To compile and run the project, you need to have Rust installed on your system. You can install Rust using the official installer, [rustup](https://rustup.rs/). On a Unix-like system, you can run the following command in your terminal:

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

**IMPORTANT NOTE:** In order to reproduce the experimental results, you need to be on a Linux system. The library itself works on any platform supported by Rust, but the experiments rely on performance measurements that only work on Linux. On non-Linux systems, the library will still compile and run, but the experiments will not produce meaningful results regarding memory usage. A warning will be printed if you attempt to run the experiments on a non-Linux system.

## Best way to have a look at the API

You can explore the API documentation generated by Rust's documentation tool, `rustdoc`. To build and view the documentation locally, run the following command in the project root directory:

```bash
cargo doc --open
```

The project is heavily documented with doc comments, so this will give you a full overview of the available functions, types, modules and technical choices made in the implementation.

> The `--open` flag might not work as expected, in that case you can manually open the generated documentation in .html with your browser. The output of the command will tell you where the documentation is located.

The library is structured into two main API layers to serve different use cases:

### High-Level API: The `solvers` Module

The `solvers` module provides the primary, user-facing API. It contains two main functions:
- `lanczos`: The standard one-pass Lanczos algorithm.
- `lanczos_two_pass`: The memory-efficient two-pass variant.

These solvers abstract away the internal mechanics of the Lanczos iteration and are the recommended entry point for most applications.

### Low-Level API: The `algorithms` Module

The `algorithms` module exposes the core building blocks of the Lanczos iterations. This API is intended for advanced use cases and testing purposes where fine-grained control is necessary. It allows for:
- Direct access to intermediate data, such as the Lanczos basis matrix $\mathbf{V}_k$.
- Implementation of custom logic between the two passes of the two-pass algorithm.
- Benchmarking specific parts of the algorithm.

While this module is public, for general use, we strongly recommend using the high-level solvers, as they provide a safer and more convenient interface.

## Compilation and Optimization

### Building the Project

Compile the project with full optimizations:

```bash
cargo build --release
```

The release profile includes aggressive optimization settings:

- **Full Link-Time Optimization (LTO)**: `lto = "fat"` enables cross-crate optimization for maximum performance
- **Single Compilation Unit**: `codegen-units = 1` forces the optimizer to consider the entire crate context
- **Runtime Optimizations**: `opt-level = 3` enables all compiler optimizations
- **Debug Information**: Retained in release builds for meaningful stack traces during long computations
- **Disabled Overflow Checks**: Standard for high-performance numerical code

> Compile time may be longer due to LTO and single codegen unit settings, but the resulting binaries will be significantly faster. If you need faster compile times, just comment out the relevant lines in `Cargo.toml` or build without `--release`. Note that debug builds will be significantly slower for numerical workloads

## Dependencies

### Core Linear Algebra: Faer

The implementation is built on the [faer](https://github.com/sarah-ek/faer-rs) library, a modern Rust linear algebra framework that provides:

- High-performance BLAS/LAPACK operations
- Memory-efficient sparse matrix representations
- Stack-allocated temporary buffers for memory management
- Matrix-free linear operators for large-scale computations

Other dependencies include:
- **Clap**: Command-line interface framework
- **CSV**: Data export for experimental results
- **Serde**: Serialization for structured data output
- **Anyhow**: Error handling and context propagation
- **Rand**: Deterministic random number generation for reproducible experiments

## Build Script (build.rs)

The `build.rs` script automatically generates test suites by scanning the `data/` directory for test instances. It performs the following operations:

1. **Instance Discovery**: Scans directories `data/1000`, `data/2000`, and `data/3000` for `.dmx` and `.qfc` file pairs
2. **Test Generation**: Creates parameterized test functions for each discovered instance
3. **Property Validation**: Generates tests for four mathematical properties:
   - **Decomposition Consistency**: Validates the Lanczos tridiagonal decomposition
   - **Lanczos Relation**: Verifies the fundamental recurrence relation
   - **Orthonormality**: Checks orthogonality of the Krylov basis vectors
   - **Reconstruction Stability**: Tests numerical stability of the two-pass reconstruction

The generated tests are included at compile time via the `include!` macro in `tests/correctness.rs`.

> The build.rs is automatically executed during the build process and does not require manual invocation.

## Repository Structure

```bash
two-pass-lanczos/
├── Cargo.toml                  # Project configuration and dependencies
├── build.rs                    # Automated test generation
├── src/
│   ├── lib.rs                  # Core library exports
│   ├── error.rs                # Error types and handling
│   ├── solvers.rs              # Lanczos algorithm implementations
│   ├── algorithms/             # Lanczos variants and utilities
│   │   ├── mod.rs
│   │   ├── lanczos.rs          # Standard one-pass Lanczos
│   │   └── lanczos_two_pass.rs # Memory-efficient two-pass Lanczos
│   ├── bin/                    # Experiment executables
│   │   ├── datagen.rs          # Data generation orchestrator
│   │   ├── scalability.rs      # Performance scaling analysis
│   │   ├── stability.rs        # Numerical accuracy experiments
│   │   ├── tradeoff.rs         # Memory-time tradeoff analysis
│   │   ├── dense_tradeoff.rs   # Dense matrix tradeoff analysis
│   │   └── orthogonality.rs    # Basis orthogonality analysis
│   └── utils/                  # Utility modules
│       ├── mod.rs
│       ├── data_loader.rs      # KKT system file parsing
│       └── perf.rs             # Performance measurement utilities
├── tests/
│   └── correctness.rs          # Mathematical correctness validation
├── data/                       # Test instance datasets
│   ├── 1000/                   # 1000-node network problems
│   ├── 2000/                   # 2000-node network problems
│   ├── 3000/                   # 3000-node network problems
│   ├── netgen/                 # DIMACS network generator
│   └── qcnd/                   # Quadratic cost generation tools
├── python/                     # Visualization and analysis scripts
│   ├── plot_scalability.py
│   ├── plot_stability.py
│   ├── plot_tradeoff.py
│   ├── plot_dense_tradeoff.py
│   └── plot_orthogonality.py
├── results/                    # Experimental output data
│   └── images/                 # Generated plots and figures
└── tex/                        # LaTeX report documentation
    ├── report.tex
    ├── report.pdf
    └── ref.bib
```

## Data Generation

The `datagen` binary orchestrates a three-stage pipeline to generate KKT test instances using external tools.

### Usage

```bash
cargo run --release --bin datagen -- \
    --arcs 5000 \
    --rho 3 \
    --instance-id 1 \
    --fixed-cost a \
    --quadratic-cost a \
    --scaling ns \
    --output-dir data/custom
```

#### Parameters

- **arcs**: Number of arcs in the generated network
- **rho**: Structural parameter (1-3) controlling network topology
- **instance-id**: Unique identifier used as random seed
- **fixed-cost**: Cost level for fixed costs (`a` = high, `b` = low)
- **quadratic-cost**: Cost level for quadratic costs (`a` = high, `b` = low)
- **scaling**: Arc capacity scaling (`s` = scaled, `ns` = not scaled)

#### Generation Pipeline

1. **pargen**: Generates parameter file with problem specifications
2. **netgen**: Creates network topology in DIMACS format (.dmx)
3. **qfcgen**: Generates quadratic cost coefficients (.qfc)

### Example: Generate Multiple Instances

```bash
# Generate instances with different structural parameters
for rho in 1 2 3; do
    for instance in 1 2 3; do
        cargo run --release --bin datagen -- \
            --arcs 10000 \
            --rho $rho \
            --instance-id $instance \
            --fixed-cost a \
            --quadratic-cost a \
            --scaling ns \
            --output-dir data/custom_10k
    done
done
```

## Correctness Testing

The test suite validates properties of the Lanczos implementations. For this test we use the datasets downloaded from the [CommaLab MCF repository](https://commalab.di.unipi.it/datasets/mcf/).

Particularly, we test _all_ the instances in the directories:

- `data/1000/` - Single-commodity problems with 1000 nodes, downloaded from [here](https://commalab.di.unipi.it/files/Data/MCF/1000.tgz)
- `data/2000/` - Single-commodity problems with 2000 nodes, downloaded from [here](https://commalab.di.unipi.it/files/Data/MCF/2000.tgz)
- `data/3000/` - Single-commodity problems with 3000 nodes, downloaded from [here](https://commalab.di.unipi.it/files/Data/MCF/3000.tgz)

You can download and extract these datasets using the following commands:

```bash
mkdir -p data/1000 data/2000 data/3000
wget https://commalab.di.unipi.it/files/Data/MCF/1000.tgz -O - | tar -xz -C data/1000 --strip-components=1
wget https://commalab.di.unipi.it/files/Data/MCF/2000.tgz -O - | tar -xz -C data/2000 --strip-components=1
wget https://commalab.di.unipi.it/files/Data/MCF/3000.tgz -O - | tar -xz -C data/3000 --strip-components=1
```

### Running All Tests

```bash
cargo test --release
```

### Test Categories

The tests are automatically generated by `build.rs` and cover four fundamental properties:

1. **Decomposition Consistency**: Validates that the tridiagonal matrix `T_k` correctly represents the projection of `A` onto the Krylov subspace
2. **Lanczos Relation**: Verifies the fundamental recurrence $A V_k = V_{k+1} \bar{T}_k$
3. **Orthonormality**: Checks that the Krylov basis vectors maintain orthogonality
4. **Reconstruction Stability**: Tests that the two-pass method reproduces the same basis as the standard method

### Test Failures

Some tests may fail due to invalid indices in downloaded datasets. These failures are not algorithmic errors but data quality issues in the source datasets. The problematic datasets are primarily in directories:

- `data/1000/` - Single-commodity problems with 1000 nodes
- `data/2000/` - Single-commodity problems with 2000 nodes
- `data/3000/` - Single-commodity problems with 3000 nodes

Valid instances can be generated using the `datagen` utility as described above.

### Running Specific Test Categories

```bash
# Run only orthonormality tests
cargo test orthonormality --release

# Run tests for a specific instance size
cargo test netgen_1000 --release
```

## Reproducing Experimental Results

The project includes three experimental analyses corresponding to the results in `tex/report.pdf`. Each experiment generates CSV data that is then visualized using Python scripts.

### 1. Memory and Computation Trade-off

This experiment validates the theoretical memory-computation trade-off by analyzing how memory usage and execution time vary with iteration count across different problem scales. The main analysis is performed on three KKT system instances of different sizes:

- Large-scale: 500,000 arcs
- Medium-scale: 50,000 arcs
- Small-scale: 5,000 arcs

All instances use `rho=3` as the structural parameter for network topology generation.

**Generate Data:**
```bash
cargo run --release --bin datagen -- \
    --arcs 5000 \
    --rho 3 \
    --instance-id 1 \
    --fixed-cost a \
    --quadratic-cost a \
    --scaling ns \
    --output-dir data/arcs5k_rho3

cargo run --release --bin datagen -- \
    --arcs 50000 \
    --rho 3 \
    --instance-id 1 \
    --fixed-cost a \
    --quadratic-cost a \
    --scaling ns \
    --output-dir data/arcs50k_rho3

cargo run --release --bin datagen -- \
    --arcs 500000 \
    --rho 3 \
    --instance-id 1 \
    --fixed-cost a \
    --quadratic-cost a \
    --scaling ns \
    --output-dir data/arcs500k_rho3
```
**Run Experiments**
```bash
# Large-scale problem (500K arcs)
cargo run --release --bin tradeoff -- \
    --instance-dir data/arcs500k_rho3 \
    --output results/tradeoff_arcs500k_rho3.csv \
    --k-start 50 \
    --k-end 1000 \
    --k-step 50

# Medium-scale problem (50K arcs)
cargo run --release --bin tradeoff -- \
    --instance-dir data/arcs50k_rho3 \
    --output results/tradeoff_arcs50k_rho3.csv \
    --k-start 50 \
    --k-end 1000 \
    --k-step 50

# Small-scale problem (5K arcs)
cargo run --release --bin tradeoff -- \
    --instance-dir data/arcs5k_rho3 \
    --output results/tradeoff_arcs5k_rho3.csv \
    --k-start 50 \
    --k-end 1000 \
    --k-step 50
```

**Generate Plots:**
```bash
python3 python/plot_tradeoff.py \
    --input results/tradeoff_arcs500k_rho3.csv \
    --output-prefix results/images/tradeoff_arcs500k_rho3

python3 python/plot_tradeoff.py \
    --input results/tradeoff_arcs50k_rho3.csv \
    --output-prefix results/images/tradeoff_arcs50k_rho3

python3 python/plot_tradeoff.py \
    --input results/tradeoff_arcs5k_rho3.csv \
    --output-prefix results/images/tradeoff_arcs5k_rho3
```

This generates six plots total: memory and time plots for each of the three problem scales.

#### Dense Matrix Validation

We also run the same experiment on a dense random matrix, where the $O(n^2)$ cost of the matrix-vector product dominates. This helps isolate the effect of memory access patterns and cache behavior.

**Generate Results:**
```bash
cargo run --release --bin dense_tradeoff -- \
    --n 10000 \
    --k-start 100 \
    --k-end 1000 \
    --k-step 100 \
    --output results/dense_tradeoff.csv
```

**Generate Plot:**
```bash
python3 python/plot_dense_tradeoff.py \
    --input results/dense_tradeoff.csv \
    --output results/images/dense_tradeoff.pdf
```

This produces a single plot showing execution time versus iteration count for both Lanczos variants on a dense matrix.

### 2. Scalability with Problem Dimension

This experiment measures how memory usage and execution time scale with problem dimension while keeping the number of iterations fixed.

**Generate Data and Results:**
```bash
cargo run --release --bin scalability -- \
    --arcs-start 5000 \
    --arcs-end 500000 \
    --arcs-step 5000 \
    --rho 3 \
    --k-fixed 500 \
    --output results/scalability_k500_rho3.csv
```

**Generate Plots:**
```bash
python3 python/plot_scalability.py \
    --input results/scalability_k500_rho3.csv \
    --output-prefix results/images/scalability_k500_rho3
```

This generates two plots showing memory usage and execution time vs problem dimension, with k=500 iterations fixed across all problem sizes.

### 3. Dense Matrix Performance Validation

This experiment validates the theoretical performance analysis by measuring execution time on dense matrices, where the O(n²) matrix-vector product cost dominates computation. This confirms that sparse matrix performance characteristics are attributable to memory access patterns rather than algorithmic differences.

**Generate Data:**
```bash
cargo run --release --bin dense_tradeoff -- \
    --n 5000 \
    --k-start 50 \
    --k-end 1000 \
    --k-step 50 \
    --output results/dense_tradeoff.csv
```

**Generate Plots:**
```bash
python3 python/plot_dense_tradeoff.py \
    --input results/dense_tradeoff.csv \
    --output results/images/dense_tradeoff.pdf
```

This generates a single plot showing execution time vs iteration count for both Lanczos variants on a dense matrix.

### 4. Numerical Accuracy and Stability Analysis

This experiment provides accuracy measurements and orthogonality studies across multiple problem scenarios. It uses two executables (`stability` and `orthogonality`) to generate four distinct experimental configurations.

**Problem Scenarios:**
The analysis tests two functions on two spectral conditions:
- **Functions:**
  - `inv`: inverse $f(z) = z^{-1}$ (linear system solving)
  - `exp`: exponential $f(z) = \exp(z)$
- **Spectral Conditions:**
  - `well-conditioned`: Eigenvalues well-separated from function singularities
  - `ill-conditioned`: Wide spectrum or eigenvalues near singularities

This creates four combinations

**Generate Accuracy Results:**
```bash
# Matrix inverse - well-conditioned spectrum
cargo run --release --bin stability -- \
    --function inv \
    --scenario well-conditioned \
    --n 10000 \
    --k-min 10 \
    --k-max 200 \
    --k-step 10 \
    --output results/accuracy_inv_well-conditioned.csv

# Matrix inverse - ill-conditioned spectrum
cargo run --release --bin stability -- \
    --function inv \
    --scenario ill-conditioned \
    --n 10000 \
    --k-min 10 \
    --k-max 200 \
    --k-step 10 \
    --output results/accuracy_inv_ill-conditioned.csv

# Matrix exponential - well-conditioned spectrum
cargo run --release --bin stability -- \
    --function exp \
    --scenario well-conditioned \
    --n 10000 \
    --k-min 10 \
    --k-max 200 \
    --k-step 10 \
    --output results/accuracy_exp_well-conditioned.csv

# Matrix exponential - ill-conditioned spectrum
cargo run --release --bin stability -- \
    --function exp \
    --scenario ill-conditioned \
    --n 10000 \
    --k-min 10 \
    --k-max 200 \
    --k-step 10 \
    --output results/accuracy_exp_ill-conditioned.csv
```

**Generate Orthogonality Results:**
```bash
# Matrix inverse - well-conditioned spectrum
cargo run --release --bin orthogonality -- \
    --function inv \
    --scenario well-conditioned \
    --n 10000 \
    --k-min 20 \
    --k-max 500 \
    --k-step 20 \
    --output results/orthogonality_inv_well-conditioned.csv

# Matrix inverse - ill-conditioned spectrum
cargo run --release --bin orthogonality -- \
    --function inv \
    --scenario ill-conditioned \
    --n 10000 \
    --k-min 20 \
    --k-max 500 \
    --k-step 20 \
    --output results/orthogonality_inv_ill-conditioned.csv

# Matrix exponential - well-conditioned spectrum
cargo run --release --bin orthogonality -- \
    --function exp \
    --scenario well-conditioned \
    --n 10000 \
    --k-min 20 \
    --k-max 500 \
    --k-step 20 \
    --output results/orthogonality_exp_well-conditioned.csv

# Matrix exponential - ill-conditioned spectrum
cargo run --release --bin orthogonality -- \
    --function exp \
    --scenario ill-conditioned \
    --n 10000 \
    --k-min 20 \
    --k-max 500 \
    --k-step 20 \
    --output results/orthogonality_exp_ill-conditioned.csv
```

**Generate All Plots:**
```bash
# Accuracy plots
python3 python/plot_stability.py \
    --input results/accuracy_inv_well-conditioned.csv \
    --output results/images/accuracy_inv_well-conditioned.pdf

python3 python/plot_stability.py \
    --input results/accuracy_inv_ill-conditioned.csv \
    --output results/images/accuracy_inv_ill-conditioned.pdf

python3 python/plot_stability.py \
    --input results/accuracy_exp_well-conditioned.csv \
    --output results/images/accuracy_exp_well-conditioned.pdf

python3 python/plot_stability.py \
    --input results/accuracy_exp_ill-conditioned.csv \
    --output results/images/accuracy_exp_ill-conditioned.pdf

# Orthogonality plots
python3 python/plot_orthogonality.py \
    --input results/orthogonality_inv_well-conditioned.csv \
    --output results/images/orthogonality_inv_well-conditioned.pdf

python3 python/plot_orthogonality.py \
    --input results/orthogonality_inv_ill-conditioned.csv \
    --output results/images/orthogonality_inv_ill-conditioned.pdf

python3 python/plot_orthogonality.py \
    --input results/orthogonality_exp_well-conditioned.csv \
    --output results/images/orthogonality_exp_well-conditioned.pdf

python3 python/plot_orthogonality.py \
    --input results/orthogonality_exp_ill-conditioned.csv \
    --output results/images/orthogonality_exp_ill-conditioned.pdf
```
